{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d902d78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.interpolate import LinearNDInterpolator\n",
    "import plotly.express as px\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80b3bc83",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1267/1267 [27:07:38<00:00, 77.08s/it]\n"
     ]
    }
   ],
   "source": [
    "# GOES FROM COLLOCATED MSS TO MONTLY OBSERVATIONS OF MSS ANOMALY GRIDDED TO 1x1\n",
    "directory = \"C:/Users/syversk/Desktop/mss_v3.0/\"\n",
    "files = os.listdir(directory + \"mss_collocated_v3.0_new\")\n",
    "df_list = []\n",
    "for i in tqdm(range(len(files))):\n",
    "    df = pd.read_csv(directory + \"mss_collocated_v3.0_new\" +\"/\" + files[i])\n",
    "    df = df[df['era_wind'] < 11]\n",
    "    df = df[df['era_wind'] > 3]\n",
    "    if \"Var5\" in df:\n",
    "        print(\"Var5\")\n",
    "        df = df.drop(['Var5'], axis=1)\n",
    "    df = df.dropna()\n",
    "    df[\"delta\"] = df.apply(lambda row: np.sqrt((row.era_u10 - row.oscar_u)**2 + (row.era_v10 - row.oscar_v)**2), axis = 1)\n",
    "    df = calculate_grided_mss_anomaly_for_each_day_df(df)\n",
    "    df_list.append(df)\n",
    "    # New month check\n",
    "    if (i < len(files) -1) and files[i][0:7] != files[i+1][0:7]:\n",
    "        df_month = pd.concat(df_list)\n",
    "        df_month = group_dataframe_temporally(df_month)\n",
    "        df_month.to_csv( directory + \"monthly_mss_ano/\" + files[i][0:7] + \".csv\" ,index=False)\n",
    "        df_list = []\n",
    "    # Last file check\n",
    "    if (i == len(files) -1):\n",
    "        df_month = pd.concat(df_list)\n",
    "        df_month = group_dataframe_temporally(df_month)\n",
    "        df_month.to_csv( directory + \"monthly_mss_ano/\" + files[i][0:7] + \".csv\" ,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aef243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRIOR TO FITTING EMPIRICAL RELATHIONSHIP BETWEEN WIND AND MSS\n",
    "\n",
    "directory = \"C:/Users/syversk/Desktop/mss_v3.0/\"\n",
    "files = os.listdir(directory + \"mss_collocated_v3.0_new\")\n",
    "for file in tqdm(files):\n",
    "    df = pd.read_csv(directory + \"mss_collocated_v3.0_new\" +\"/\" + file)\n",
    "    \n",
    "    df = remove_extra(df)\n",
    "        \n",
    "    df_cr1 = reduce_area_of_df(df, \"cr1\")\n",
    "    df_cr1.to_csv( directory + \"cr1/\" + file ,index=False)\n",
    "    \n",
    "    df_cr2 = reduce_area_of_df(df, \"cr2\")\n",
    "    df_cr2.to_csv( directory + \"cr2/\" + file ,index=False)\n",
    "    \n",
    "    df_cr1_poly = reduce_area_of_df_poly(df, \"north\")\n",
    "    df_cr1_poly.to_csv( directory + \"cr1_north_pasific/\" + file ,index=False)\n",
    "    \n",
    "    df_cr2_poly = reduce_area_of_df_poly(df, \"south\")\n",
    "    df_cr2_poly.to_csv( directory + \"cr2_south_pasific/\" + file ,index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73d43ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_area_of_df_poly(input_df, cr):\n",
    "    directory = \"C:/Users/syversk/Desktop/\"\n",
    "    if cr == \"south\":\n",
    "        df = pd.read_csv(directory + \"low_microplastics_region_south_pacific.csv\")\n",
    "    else:\n",
    "        df = pd.read_csv(directory + \"low_microplastics_region_north_pacific.csv\")\n",
    "    tup_list = []\n",
    "    for row in df.iterrows():\n",
    "        #Plus 360 cause both regions lie in negative longtiude according to def.\n",
    "        tup_list.append((row[1][0]+360, row[1][1]))\n",
    "    polygon = Polygon(tup_list)\n",
    "    \n",
    "    input_df[\"in_poly\"] = input_df.apply(lambda row: polygon.contains(Point(row.lon, row.lat)), axis = 1)\n",
    "    input_df = input_df[input_df.in_poly == True]\n",
    "    input_df = input_df.drop(['in_poly'], axis=1)\n",
    "    return input_df\n",
    "\n",
    "def calculate_grided_mss_anomaly_for_each_day_df(df):\n",
    "    df = mss_katzberg(df, True)\n",
    "    df = mss_katzberg(df, False)\n",
    "    \n",
    "    df = mss_anomaly_gbdt(df, True)\n",
    "    df = mss_anomaly_gbdt(df, False)\n",
    "    \n",
    "    df = make_data_grided(df)\n",
    "    df = group_dataframe_temporally(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def mss_katzberg(df, self_trained):\n",
    "    \n",
    "    if self_trained:\n",
    "        colmn_name_1 = \"mss_ano_w_refitted\" \n",
    "        colmn_name_2 = \"mss_ano_d_refitted\"\n",
    "    else:\n",
    "        colmn_name_1 = \"mss_ano_w_towards\" \n",
    "        colmn_name_2 = \"mss_ano_d_towards\"\n",
    "        \n",
    "    df[colmn_name_1] = df.apply(lambda row: calculate_mss_anomaly(row.mss, row.era_wind, self_trained), axis = 1)\n",
    "    df[colmn_name_2] = df.apply(lambda row: calculate_mss_anomaly(row.mss, row.delta, self_trained), axis = 1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def mss_anomaly_gbdt(df, pasific = True):\n",
    "    \n",
    "    wind = df[\"era_wind\"].to_numpy()\n",
    "    delta = df[\"delta\"].to_numpy()\n",
    "    wind = wind.reshape((wind.shape[0], 1))\n",
    "    delta = delta.reshape((wind.shape[0], 1))  \n",
    "    \n",
    "    if pasific:\n",
    "        # MODEL BASED ON CR1 AND CR2\n",
    "        model = joblib.load(\"v3.0_results/model_cr_pasific.pkl\")\n",
    "    else:\n",
    "        # THIS IS BASED ON CR SOUTH AND NORTH PACSIFIC SUGGESTED BY MOSTAFA\n",
    "        model = joblib.load(\"v3.0_results/model_cr_towards.pkl\")\n",
    "    \n",
    "    \n",
    "    mss_wind = model.predict(wind)\n",
    "    mss_delta = model.predict(delta)\n",
    "        \n",
    "    mss_anomaly_wind = (df['mss']-mss_wind)/mss_wind\n",
    "    mss_anomaly_delta = (df['mss']-mss_delta)/mss_delta\n",
    "    if pasific:\n",
    "        df['mss_ano_w_gdt_pasific'] = mss_anomaly_wind\n",
    "        df['mss_ano_d_gdt_pasific'] = mss_anomaly_delta\n",
    "    else:\n",
    "        df['mss_ano_w_gdt_towards_cr'] = mss_anomaly_wind\n",
    "        df['mss_ano_d_gdt_towards_cr'] = mss_anomaly_delta\n",
    "    return df\n",
    "    \n",
    "def calculate_mss_anomaly(mss, wind_or_delta, self_trained):\n",
    "    if self_trained:\n",
    "        # THIS IS BASED ON CR1 AND CR2\n",
    "        c = [0.00370243 , 1.07334119 , 1.33996524 , 5.66160183 , -1.01960507]\n",
    "        \n",
    "        \n",
    "        # THIS IS BASED ON CR SOUTH AND NORTH PACSIFIC SUGGESTED BY MOSTAFA V 2.1 \n",
    "        c = [0.00349525 , 0.67100802 , 3.28279911 , 6.58959564 , -2.53669967]\n",
    "    else:\n",
    "        c = [0.0035, 1, 0.62, 6, -3.39]\n",
    "    if wind_or_delta <= 3.49:\n",
    "        mss_mod = c[0]*(c[1]*wind_or_delta + c[2])\n",
    "    else:\n",
    "        mss_mod = c[0]*(c[3]*np.log(wind_or_delta) + c[4])\n",
    "    return (mss-mss_mod)/mss_mod\n",
    "\n",
    "def reduce_area_of_df(df, cr):\n",
    "    if cr == \"cr1\":\n",
    "        df = df[df.lat <= -10]\n",
    "        df = df[df.lat >= -25]\n",
    "        df = df[df.lon >= 105]\n",
    "        df = df[df.lon <= 120]\n",
    "    else:\n",
    "        df = df[df.lat >= 10]\n",
    "        df = df[df.lat <= 20]\n",
    "        df = df[df.lon >= 128]\n",
    "        df = df[df.lon <= 143]\n",
    "    return df\n",
    "\n",
    "def remove_extra(df):\n",
    "    if \"Var5\" in df:\n",
    "        df = df.drop(['Var5'], axis=1)\n",
    "    rel_wind_current = np.sqrt((df[\"era_u10\"] - df[\"oscar_u\"])**2 + (df[\"era_v10\"] - df[\"oscar_v\"])**2)\n",
    "    df.drop(['era_u10', \"oscar_u\", \"era_v10\", \"oscar_v\"], inplace=True, axis=1)\n",
    "    df[\"delta\"] = rel_wind_current\n",
    "    return df\n",
    "\n",
    "#Average spatially\n",
    "def make_data_grided(df):\n",
    "    df[\"lon\"] = df[\"lon\"].apply(lambda lon: round(lon))\n",
    "    df[\"lat\"] = df[\"lat\"].apply(lambda lat: round(lat))\n",
    "    return df\n",
    "\n",
    "def interp_microplastics(df):\n",
    "    directory = \"C:/Users/syversk/Desktop\"\n",
    "    micro_df = pd.read_csv(directory + \"/\" + \"micro_df.csv\")\n",
    "    interp_micro_mass = LinearNDInterpolator(list(zip(mic_df['lon'], mic_df['lat'])), mic_df['vansebillemodel_mass_log'])\n",
    "    interp_micro_abu = LinearNDInterpolator(list(zip(mic_df['lon'], mic_df['lat'])), mic_df['vansebillemodel_abundance_log'])\n",
    "    df['micro_mass'] = interp_micro_mass(df['lon'], df['lat'])\n",
    "    df['abundace'] = interp_micro_abu(df['lon'], df['lat'])\n",
    "    return df\n",
    "\n",
    "#Average all temporally\n",
    "def group_dataframe_temporally(df):\n",
    "    df = df.groupby(['lon', 'lat'], as_index=False)[['mss_ano_w_gdt_pasific', 'mss_ano_d_gdt_pasific'\n",
    "                                                    , 'mss_ano_w_gdt_towards_cr', 'mss_ano_d_gdt_towards_cr'\n",
    "                                                    , 'mss_ano_w_refitted', 'mss_ano_d_refitted',\n",
    "                                                    \"mss_ano_w_towards\", \"mss_ano_d_towards\"]].mean()\n",
    "    return df\n",
    "\n",
    "def hours_to_date(d0, hours):\n",
    "    day = d0 + timedelta(hours=hours)\n",
    "    return day\n",
    "\n",
    "#Average temporally to dates\n",
    "def group_dataframe(df):\n",
    "    d0 = date(1992, 10, 5)\n",
    "    df[\"time\"] = df[\"time\"].apply(lambda time: hours_to_date(d0,time))\n",
    "    df = df.groupby(['lon', 'lat', \"time\"], as_index=False)[['mss_anomaly_delta_refitted', 'mss_anomaly_wind_refitted'\n",
    "                                                            , 'mss_anomaly_delta_towards', 'mss_anomaly_wind_towards'\n",
    "                                                            , 'mss_anomaly_delta_gdt', 'mss_anomaly_wind_gdt']].mean()\n",
    "    df = df.rename(columns={\"time\": \"date\"})\n",
    "    return df\n",
    "\n",
    "def reduce_based_on_sd(df):\n",
    "    z_scores = stats.zscore(df)\n",
    "    abs_z_scores = np.abs(z_scores)\n",
    "    filtered_entries = (abs_z_scores < 3).all(axis=1)\n",
    "    return df[filtered_entries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d563ea0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
