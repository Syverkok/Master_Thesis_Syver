{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d902d78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.interpolate import LinearNDInterpolator\n",
    "import plotly.express as px\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80b3bc83",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1681/1681 [1:11:06<00:00,  2.54s/it]\n"
     ]
    }
   ],
   "source": [
    "# GOES FROM COLLOCATED MSS TO MONTLY OBSERVATIONS OF MSS ANOMALY GRIDDED TO 1x1\n",
    "directory = \"C:/Users/syversk/Desktop/mss_collocated\"\n",
    "files = os.listdir(directory)\n",
    "df_list = []\n",
    "for i in tqdm(range(len(files))):\n",
    "    df = pd.read_csv(directory + \"/\" + files[i])\n",
    "    if \"Var5\" in df:\n",
    "        df = df.drop(['Var5'], axis=1)\n",
    "    df = df.dropna()\n",
    "    df = calculate_grided_mss_anomaly_for_each_day_df_GDT(df)\n",
    "    df_list.append(df)\n",
    "    # New month check\n",
    "    if (i < len(files) -1) and files[i][0:7] != files[i+1][0:7]:\n",
    "        df = pd.concat(df_list)\n",
    "        df = group_dataframe_temporally(df)\n",
    "        df.to_csv( \"C:/Users/syversk/Desktop/GBDT/\" + files[i][0:7] + \".csv\" ,index=False)\n",
    "        df_list = []\n",
    "    # Last file check\n",
    "    if (i == len(files) -1):\n",
    "        df = pd.concat(df_list)\n",
    "        df = group_dataframe_temporally(df)\n",
    "        df.to_csv( \"C:/Users/syversk/Desktop/GBDT/\" + files[i][0:7] + \".csv\" ,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73d43ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_df(df):\n",
    "    df = remove_extra(df)\n",
    "    df.to_csv( \"C:/Users/syversk/Desktop/mss_collocated/\" + file ,index=False)\n",
    "    df_cr1 = reduce_area_of_df(df, \"cr1\")\n",
    "    df_cr1.to_csv( \"C:/Users/syversk/Desktop/cr1/\" + file ,index=False)\n",
    "    df = reduce_area_of_df(df, \"cr2\")\n",
    "    df.to_csv( \"C:/Users/syversk/Desktop/cr2/\" + file ,index=False)\n",
    "    \n",
    "def calculate_grided_mss_anomaly_for_each_day_df(df):\n",
    "    df = df[df['era_wind'] < 11]\n",
    "    df = df[df['era_wind'] > 3]\n",
    "    df['mss_anomaly_wind'] = df.apply(lambda row: calculate_mss_anomaly(row.mss, row.era_wind, self_trained = False), axis = 1)\n",
    "    df['mss_anomaly_delta'] = df.apply(lambda row: calculate_mss_anomaly(row.mss, row.delta, self_trained = False), axis = 1)\n",
    "    df = make_data_grided(df)\n",
    "    df = group_dataframe_temporally(df)\n",
    "    return df\n",
    "\n",
    "def calculate_grided_mss_anomaly_for_each_day_df_GDT(df):\n",
    "    df = df[df['era_wind'] < 11]\n",
    "    df = df[df['era_wind'] > 3]\n",
    "    wind = df[\"era_wind\"].to_numpy()\n",
    "    delta = df[\"delta\"].to_numpy()\n",
    "    wind = wind.reshape((wind.shape[0], 1))\n",
    "    delta = delta.reshape((wind.shape[0], 1))  \n",
    "    use_scaler = False\n",
    "    if use_scaler:\n",
    "        model = joblib.load(\"model_full_data.pkl\")\n",
    "        scaler=joblib.load('std_scaler.bin')\n",
    "        rescaled_wind = scaler.transform(wind)\n",
    "        rescaled_delta = scaler.transform(delta)\n",
    "        mss_wind = model.predict(rescaled_wind)\n",
    "        mss_delta = model.predict(rescaled_delta)\n",
    "    else:\n",
    "        model = joblib.load(\"model_full_dataFalse.pkl\")\n",
    "        mss_wind = model.predict(wind)\n",
    "        mss_delta = model.predict(delta)\n",
    "    \n",
    "    mss_anomaly_wind = (df['mss']-mss_wind)/mss_wind\n",
    "    mss_anomaly_delta = (df['mss']-mss_delta)/mss_delta\n",
    "    df['mss_anomaly_wind'] = mss_anomaly_wind\n",
    "    df['mss_anomaly_delta'] = mss_anomaly_delta\n",
    "    df = make_data_grided(df)\n",
    "    df = group_dataframe_temporally(df)\n",
    "    return df\n",
    "\n",
    "def calculate_mss_anomaly(mss, wind_or_delta, self_trained = False):\n",
    "    if self_trained:\n",
    "        c = [0.00370243 , 1.07334119 , 1.33996524 , 5.66160183 , -1.01960507]\n",
    "    else:\n",
    "        c = [0.0035, 1, 0.62, 6, -3.39]\n",
    "    if wind_or_delta <= 3.49:\n",
    "        mss_mod = c[0]*(c[1]*wind_or_delta + c[2])\n",
    "    else:\n",
    "        mss_mod = c[0]*(c[3]*np.log(wind_or_delta) + c[4])\n",
    "    return (mss-mss_mod)/mss_mod\n",
    "\n",
    "def reduce_area_of_df(df, cr):\n",
    "    if cr == \"cr1\":\n",
    "        df = df[df.lat <= -10]\n",
    "        df = df[df.lat >= -25]\n",
    "        df = df[df.lon >= 105]\n",
    "        df = df[df.lon <= 120]\n",
    "    else:\n",
    "        df = df[df.lat >= 10]\n",
    "        df = df[df.lat <= 20]\n",
    "        df = df[df.lon >= 128]\n",
    "        df = df[df.lon <= 143]\n",
    "    return df\n",
    "\n",
    "def remove_extra(df):\n",
    "    rel_wind_current = np.sqrt((df[\"era_u10\"] - df[\"oscar_u\"])**2 + (df[\"era_v10\"] - df[\"oscar_v\"])**2)\n",
    "    df.drop(['era_u10', \"oscar_u\", \"era_v10\", \"oscar_v\", \"oscar_current\"], inplace=True, axis=1)\n",
    "    df[\"delta\"] = rel_wind_current\n",
    "    return df\n",
    "\n",
    "#Average spatially\n",
    "def make_data_grided(df):\n",
    "    df[\"lon\"] = df[\"lon\"].apply(lambda lon: round(lon))\n",
    "    df[\"lat\"] = df[\"lat\"].apply(lambda lat: round(lat))\n",
    "    return df\n",
    "\n",
    "def interp_microplastics(df):\n",
    "    directory = \"C:/Users/syversk/Desktop\"\n",
    "    micro_df = pd.read_csv(directory + \"/\" + \"micro_df.csv\")\n",
    "    interp_micro_mass = LinearNDInterpolator(list(zip(mic_df['lon'], mic_df['lat'])), mic_df['vansebillemodel_mass_log'])\n",
    "    interp_micro_abu = LinearNDInterpolator(list(zip(mic_df['lon'], mic_df['lat'])), mic_df['vansebillemodel_abundance_log'])\n",
    "    df['micro_mass'] = interp_micro_mass(df['lon'], df['lat'])\n",
    "    df['abundace'] = interp_micro_abu(df['lon'], df['lat'])\n",
    "    return df\n",
    "\n",
    "#Average all temporally\n",
    "def group_dataframe_temporally(df):\n",
    "    df = df.groupby(['lon', 'lat'], as_index=False)[['mss_anomaly_delta', 'mss_anomaly_wind']].mean()\n",
    "    return df\n",
    "\n",
    "def hours_to_date(d0, hours):\n",
    "    day = d0 + timedelta(hours=hours)\n",
    "    return day\n",
    "\n",
    "#Average temporally to dates\n",
    "def group_dataframe(df):\n",
    "    d0 = date(1992, 10, 5)\n",
    "    df[\"time\"] = df[\"time\"].apply(lambda time: hours_to_date(d0,time))\n",
    "    df = df.groupby(['lon', 'lat', \"time\"], as_index=False)[['mss_anomaly_delta', 'mss_anomaly_wind']].mean()\n",
    "    df = df.rename(columns={\"time\": \"date\"})\n",
    "    return df\n",
    "\n",
    "def reduce_based_on_sd(df):\n",
    "    z_scores = stats.zscore(df)\n",
    "    abs_z_scores = np.abs(z_scores)\n",
    "    filtered_entries = (abs_z_scores < 3).all(axis=1)\n",
    "    return df[filtered_entries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c7aab0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
