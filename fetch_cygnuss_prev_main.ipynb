{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydap.client import open_url\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import pyproj\n",
    "import xarray as xr\n",
    "from scipy.interpolate import LinearNDInterpolator\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pickle\n",
    "from datetime import date\n",
    "import os\n",
    "import plotly.express as px\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
    "import matplotlib\n",
    "import cartopy as cart\n",
    "\n",
    "def retrieve_microplastics_gt():\n",
    "    lats = np.zeros(181)\n",
    "    count = 90\n",
    "    for i in range(len(lats)):\n",
    "        lats[i] = count\n",
    "        count -= 1\n",
    "    for filename in os.listdir('microplast_gt'):\n",
    "        logfilename = filename[:-4] + '_log'\n",
    "        try:\n",
    "            df = pd.read_csv('microplast_gt/' + filename, header=None)\n",
    "        except:\n",
    "            print(filename)\n",
    "            continue\n",
    "        longlist = []\n",
    "        latlist = []\n",
    "        valuelist_log = []\n",
    "        valuelist = []\n",
    "        for long in range(0, 361):\n",
    "            for lat in range(0, 181):\n",
    "                val = df[long][lat]\n",
    "                if val < 1:\n",
    "                    val = np.nan\n",
    "                longlist.append(long)\n",
    "                latlist.append(lats[lat])\n",
    "                valuelist_log.append(np.log10(val))\n",
    "                valuelist.append(val)\n",
    "        if filename == \"lebretonmodel_abundance.csv\":\n",
    "            res_df = pd.DataFrame({'sp_lon' : longlist, 'sp_lat' : latlist, filename[:-4] : valuelist, logfilename : valuelist_log})\n",
    "        else:\n",
    "            res_df[filename[:-4]] = valuelist\n",
    "            res_df[logfilename] = valuelist_log\n",
    "    return res_df\n",
    "\n",
    "def plot_wind(df, var1, var2 ,interpolated = False):\n",
    "\n",
    "    # Settings for the plot\n",
    "    ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "    ax.coastlines()\n",
    "    lon_formatter = LongitudeFormatter()\n",
    "    lat_formatter = LatitudeFormatter()\n",
    "    ax.xaxis.set_major_formatter(lon_formatter)\n",
    "    ax.yaxis.set_major_formatter(lat_formatter)\n",
    "    ax.gridlines(draw_labels=True, alpha=0.5)\n",
    "    plt.scatter(df['sp_lon'], df['sp_lat'], c=list(np.sqrt(df[var1]**2 + df[var2]**2)), cmap='RdBu')\n",
    "    bar = plt.colorbar(pad=0.15, orientation='horizontal')\n",
    "    bar.ax.set_title('m/s')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    if interpolated:\n",
    "        plt.savefig('wind_speed_inter.png')\n",
    "    else:\n",
    "        plt.savefig('wind_speed.png')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_var(df, var):\n",
    "\n",
    "    # Settings for the plot\n",
    "    ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "    ax.coastlines()\n",
    "    ax.add_feature(cart.feature.LAND, zorder=100, edgecolor='k')\n",
    "    ax.gridlines(draw_labels=True, alpha=0.5)\n",
    "    lon_formatter = LongitudeFormatter(zero_direction_label=True)\n",
    "    lat_formatter = LatitudeFormatter()\n",
    "    ax.xaxis.set_major_formatter(lon_formatter)\n",
    "    ax.yaxis.set_major_formatter(lat_formatter)\n",
    "    plt.scatter(df['sp_lon'], df['sp_lat'], c=list(df[var]))\n",
    "    bar = plt.colorbar(pad=0.15, orientation='horizontal')\n",
    "    plt.title(var)\n",
    "    bar.ax.set_title('Power 10^')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.savefig('nbrcs-diff.svg')\n",
    "    plt.show()\n",
    "    \n",
    "def generate_url(year, month, day, satellite_number):\n",
    "\n",
    "    day_of_year = datetime(year, month, day).timetuple().tm_yday\n",
    "    date_string = str(year) + str(month).zfill(2) + str(day).zfill(2)\n",
    "\n",
    "    base_url = 'https://podaac-opendap.jpl.nasa.gov/opendap/hyrax/allData/cygnss/L1/v3.0/'\n",
    "    specific_url = str(year) + '/' + str(day_of_year).zfill(3) + '/cyg0' + str(satellite_number) + '.ddmi.s' + \\\n",
    "                   date_string + '-000000-e' + date_string + '-235959.l1.power-brcs.a30.d31.nc'\n",
    "    data_url = base_url + specific_url\n",
    "    clickable_url = base_url + specific_url + '.html'\n",
    "\n",
    "    return data_url + '?sp_lat,sp_lon,track_id,quality_flags,ddm_timestamp_utc,ddm_nbrcs,fresnel_coeff,sp_inc_angle', clickable_url\n",
    "\n",
    "def fetch_cygnss(y1, m1, d1, y2, m2, d2):\n",
    "    sdate = date(y1, m1, d1)   # start date\n",
    "    edate = date(y2, m2, d2)   # end date\n",
    "    delta = edate - sdate       # as timedelta\n",
    "    df_list = []\n",
    "    for i in range(delta.days + 1):\n",
    "        day = sdate + timedelta(days=i)\n",
    "        df = get_cygnss_data(day.year, day.month, day.day)\n",
    "        df = prep_cygnss(df)\n",
    "        df_list.append(df)\n",
    "        df.to_csv(\"cygnss_data_whole_world/\" + str(day.year) + str(day.month) + str(day.day) + \".csv\" ,index=False)\n",
    "    if len(df_list) == 1:\n",
    "        return df_list[0]\n",
    "    else:\n",
    "        return pd.concat(df_list)\n",
    "    \n",
    "def hours_since_ref(year, month, day):\n",
    "    d0 = date(1992, 10, 5)\n",
    "    d1 = date(year, month, day)\n",
    "    delta = d1 - d0\n",
    "    hours = delta.days*24\n",
    "    return hours\n",
    "\n",
    "def get_cygnss_data(year, month, day):\n",
    "    cygnss_df = pd.DataFrame()\n",
    "    hours = hours_since_ref(year, month, day)\n",
    "    for sat_numb in range(1, 9):  # Remember to change back to  1, 9\n",
    "        print(\"Satellite number : \" + str(sat_numb))\n",
    "        test_data_url, test_clickable_url = generate_url(year, month, day, sat_numb)\n",
    "        dataset = open_url(test_data_url, output_grid=False)\n",
    "        for ddm in range(4):  # Remember to change back to 4\n",
    "            ddm_df = pd.DataFrame()\n",
    "            print(\"ddm : \" + str(ddm))\n",
    "\n",
    "            ddm_timestamp_utc = np.array(dataset.ddm_timestamp_utc[:, ddm])\n",
    "            ddm_timestamp_utc = np.rint(ddm_timestamp_utc / 3600) + hours\n",
    "\n",
    "            ddm_df['sp_lat'] = np.array(dataset.sp_lat[:, ddm]).tolist()\n",
    "            sp_lon = np.array(dataset.sp_lon[:, ddm]).tolist()\n",
    "            ddm_df['sp_lon'] = sp_lon\n",
    "            ddm_df['sp_lon'] = np.array(dataset.sp_lon[:, ddm]).tolist()\n",
    "            ddm_df['hours_since_ref'] = ddm_timestamp_utc.tolist()\n",
    "            ddm_df['ddm_nbrcs'] = np.array(dataset.ddm_nbrcs[:, ddm]).tolist()\n",
    "            ddm_df['fresnel_coeff'] = np.array(dataset.fresnel_coeff[:, ddm]).tolist()\n",
    "            ddm_df['sp_inc_angle'] = np.array(dataset.sp_inc_angle[:, ddm]).tolist()\n",
    "            ddm_df['quality_flags'] = np.array(dataset.quality_flags[:, ddm]).tolist()\n",
    "            ddm_df['track_id'] = np.array(dataset.track_id[:, ddm]).tolist()\n",
    "            \n",
    "            for col in ddm_df.columns:\n",
    "                if col != 'ddm_channel' and col != 'hours_since_ref' and col != 'unique_track_id':\n",
    "                    ddm_df[col] = ddm_df[col].apply(lambda x: x[0])\n",
    "            hoursince = [str(hours)] * len(sp_lon)\n",
    "            sat_numb_ar = [str(sat_numb)] * len(sp_lon)\n",
    "            ddm_df['unique_track_id'] = generate_unique_track_id_value(hoursince, list(map(str, ddm_df['track_id'])), \n",
    "                                                                       sat_numb_ar)\n",
    "            cygnss_df = cygnss_df.append(ddm_df, ignore_index=True)\n",
    "    return cygnss_df\n",
    "\n",
    "\n",
    "def generate_unique_track_id_value(hoursince, track_id, sat_nr):\n",
    "    return list(map(''.join, zip(*[hoursince, track_id, sat_nr])))\n",
    "\n",
    "def open_oskar_data_local(filename):\n",
    "    ds = xr.open_dataset(filename)\n",
    "    oskar_df = ds.to_dataframe()\n",
    "    oskar_df.dropna(inplace = True)\n",
    "    oskar_df = oskar_df.reset_index()\n",
    "    d0 = date(1992, 10, 5)\n",
    "    d1 = oskar_df['time'][1].date()\n",
    "    delta = d1 - d0\n",
    "    hours = delta.days*24\n",
    "    oskar_df['time'] = np.zeros(len(oskar_df['time']))\n",
    "    oskar_df = oskar_df.assign(time=hours)\n",
    "    oskar_df = oskar_df.rename(columns={\"latitude\": \"sp_lat\", \"longitude\": \"sp_lon\", \"time\" : \"hours_since_ref\"})\n",
    "    oskar_df = oskar_df.loc[oskar_df['sp_lon'] < 380 ]\n",
    "    oskar_df['sp_lon'] = oskar_df['sp_lon']%360\n",
    "    oskar_df = reduce_area_of_df(oskar_df)\n",
    "    return oskar_df\n",
    "\n",
    "\n",
    "def fetch_all_oskar_files():\n",
    "    df_list = []\n",
    "    for filename in os.listdir('oskar_data'):\n",
    "        df_list.append(open_oskar_data_local('oskar_data/' + filename))\n",
    "    if len(df_list) == 1:\n",
    "        return df_list[0]\n",
    "    else:\n",
    "        return pd.concat(df_list)\n",
    "\n",
    "def open_cygnss_csvs():\n",
    "    df_list = []\n",
    "    for filename in os.listdir('cygnss_data'):\n",
    "        df_list.append(pd.read_csv('cygnss_data/' + filename))\n",
    "    if len(df_list) == 1:\n",
    "        return df_list[0]\n",
    "    else:\n",
    "        return pd.concat(df_list)\n",
    "    \n",
    "def open_anomalies_csvs():\n",
    "    df_list = []\n",
    "    for filename in os.listdir('mss_ano_df'):\n",
    "        df_list.append(pd.read_csv('mss_ano_df/' + filename))\n",
    "    if len(df_list) == 1:\n",
    "        return df_list[0]\n",
    "    else:\n",
    "        return pd.concat(df_list)\n",
    "\n",
    "def prep_cygnss(cygnss_df):\n",
    "    np.warnings.filterwarnings('ignore')\n",
    "    for key in cygnss_df:\n",
    "        cygnss_df = cygnss_df[cygnss_df[key] != -9999.0]\n",
    "    cygnss_df = cygnss_df[cygnss_df.quality_flags%2 == 0]\n",
    "    cygnss_df.dropna(inplace = True)\n",
    "    cygnss_df['nbrcs_log'] = 10*np.log10(cygnss_df.ddm_nbrcs.to_numpy())\n",
    "    cygnss_df.drop('ddm_nbrcs', inplace=True, axis=1)\n",
    "    return cygnss_df\n",
    "\n",
    "# SET AREA, Function Is called when extracting CYGNSS, OSKAR and ERA5\n",
    "def reduce_area_of_df(df):\n",
    "    df = df[df.sp_lat <= 40]\n",
    "    df = df[df.sp_lat >= -40]\n",
    "    \n",
    "    df_northern = df[df.sp_lat >= 20]\n",
    "    df_southern = df[df.sp_lat <= -20]\n",
    "    df = pd.concat([df_northern,df_southern])\n",
    "    df = df[df.sp_lon >= 220]\n",
    "    return df[df.sp_lon <= 260]\n",
    "\n",
    "def get_era_5(filename):\n",
    "    ds = xr.open_dataset(filename)\n",
    "    era_5_df = ds.to_dataframe()\n",
    "    index_long = era_5_df.index.levels[0]\n",
    "    index_lat = era_5_df.index.levels[1]\n",
    "    index_time = era_5_df.index.levels[2]\n",
    "\n",
    "    start_time = pd.Timestamp(1992, 10, 5)\n",
    "    index_time_fixed = []\n",
    "    for tid in index_time:\n",
    "        hours = tid - start_time\n",
    "        index_time_fixed.append(hours.days*24 + hours.seconds/3600)\n",
    "\n",
    "    long, lat, time = np.meshgrid(index_long, index_lat, index_time_fixed, indexing = 'ij')\n",
    "\n",
    "    long = long.flatten()\n",
    "    lat = lat.flatten()\n",
    "    time = time.flatten()\n",
    "\n",
    "    return pd.DataFrame({'sp_lon': long+360, 'sp_lat' :lat, 'hours_since_ref': time, 'u10': era_5_df[\"u10\"].to_numpy()\n",
    "                         ,'v10': era_5_df[\"v10\"].to_numpy()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oskar_df = fetch_all_oskar_files()\n",
    "oskar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read = False\n",
    "if read:\n",
    "    cygnss_df = open_cygnss_csvs()\n",
    "    # If you want to change area\n",
    "    cygnss_df = reduce_area_of_df(cygnss_df)\n",
    "else:\n",
    "    cygnss_df = fetch_cygnss(2021, 11, 7, 2021, 11, 8)\n",
    "    cygnss_df = reduce_area_of_df(cygnss_df)\n",
    "cygnss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cygnss_df.unique_track_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era_5_df = get_era_5('era_5_wind/wind_one_week.nc')\n",
    "era_5_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>u10</th>\n",
       "      <th>v10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [u10, v10]\n",
       "Index: []"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = xr.open_dataset('era_5_wind/wind_one_week.nc')\n",
    "era_5_df = ds.to_dataframe()\n",
    "era_5_df\n",
    "\n",
    "def reduce_area_of_df_era_5(df):\n",
    "    df = df[df.index.get_level_values(0) + 360 >= 220]\n",
    "    df = df[df.index.get_level_values(0) + 360 <= 260]\n",
    "    df = df[df.index.get_level_values(1) <= 40]\n",
    "    df = df[df.index.get_level_values(1) >= -40]\n",
    "\n",
    "    df_northern = df[df.index.get_level_values(1) >= 20]\n",
    "    df_southern = df[df.index.get_level_values(1) <= -20]\n",
    "    return pd.concat([df_northern, df_southern])\n",
    "reduce_area_of_df_era_5(era_5_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_u10 = LinearNDInterpolator(list(zip(era_5_df['sp_lon'], era_5_df['sp_lat'], era_5_df['hours_since_ref'])), era_5_df['u10'])\n",
    "print(\"done\")\n",
    "interp_v10 = LinearNDInterpolator(list(zip(era_5_df['sp_lon'], era_5_df['sp_lat'], era_5_df['hours_since_ref'])), era_5_df['v10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lons_to_interpolate = cygnss_df[\"sp_lon\"].to_numpy()\n",
    "lats_to_interpolate = cygnss_df[\"sp_lat\"].to_numpy()\n",
    "times_to_interpolate = cygnss_df[\"hours_since_ref\"].to_numpy()\n",
    "\n",
    "u10 = interp_u10(lons_to_interpolate, lats_to_interpolate, times_to_interpolate)\n",
    "v10 = interp_v10(lons_to_interpolate, lats_to_interpolate, times_to_interpolate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cygnss_and_interp_df = pd.DataFrame({'sp_lon': lons_to_interpolate, 'sp_lat' :lats_to_interpolate, 'hours_since_ref': times_to_interpolate, 'u10': u10, 'v10': v10, 'nbrcs_log' : cygnss_df['nbrcs_log'], 'fresnel_coeff' : cygnss_df['fresnel_coeff'], 'sp_inc_angle' : cygnss_df['sp_inc_angle']  })\n",
    "cygnss_and_interp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_u = LinearNDInterpolator(list(zip(oskar_df['sp_lat'], oskar_df['sp_lon'], oskar_df['hours_since_ref'])), oskar_df['u'])\n",
    "print(\"done\")\n",
    "interp_v = LinearNDInterpolator(list(zip(oskar_df['sp_lat'], oskar_df['sp_lon'], oskar_df['hours_since_ref'])), oskar_df['v'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lons_to_interpolate = cygnss_df[\"sp_lon\"].to_numpy()\n",
    "lats_to_interpolate = cygnss_df[\"sp_lat\"].to_numpy()\n",
    "times_to_interpolate = cygnss_df[\"hours_since_ref\"].to_numpy()\n",
    "\n",
    "u_current = interp_u(lats_to_interpolate, lons_to_interpolate, times_to_interpolate)\n",
    "v_current = interp_v(lats_to_interpolate, lons_to_interpolate, times_to_interpolate)\n",
    "\n",
    "cygnss_and_interp_df['u'] = u_current\n",
    "cygnss_and_interp_df['v'] = v_current\n",
    "cygnss_and_interp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_u = cygnss_and_interp_df['u10']-cygnss_and_interp_df['u']\n",
    "diff_v = cygnss_and_interp_df['v10']-cygnss_and_interp_df['v']\n",
    "delta = np.sqrt(diff_u**2 + diff_v**2)\n",
    "\n",
    "top_frac = 0.059*delta**3 - 0.147*delta**2 + 0.903**delta + 1.389\n",
    "bot_frac = delta**3 + 18.161*delta**2 - 117.602*delta + 706.9\n",
    "mss_from_wind_current = top_frac/bot_frac\n",
    "fresnell_sqrd = cygnss_and_interp_df['fresnel_coeff'].to_numpy()**2\n",
    "\n",
    "bias_df = pd.read_csv ('bias_model.csv')\n",
    "interp_bias = LinearNDInterpolator(list(zip(bias_df['inc_angle'], bias_df['delta'])), bias_df['bias'])\n",
    "biases = interp_bias(cygnss_and_interp_df['sp_inc_angle'], delta)\n",
    "predicted_nbrcs = 10*np.log10(fresnell_sqrd/mss_from_wind_current) + biases\n",
    "\n",
    "mss_from_cygnss = fresnell_sqrd/(10**((cygnss_and_interp_df['nbrcs_log'] - biases)/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CYGNSS - CURRENT AND WIND ESTIMATIONS\n",
    "cygnss_and_interp_df['diff_nbrcs'] = cygnss_and_interp_df['nbrcs_log'] - predicted_nbrcs\n",
    "cygnss_and_interp_df['diff_mss'] = (mss_from_cygnss - mss_from_wind_current)/mss_from_wind_current\n",
    "cygnss_and_interp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mic_df = retrieve_microplastics_gt().dropna()\n",
    "mic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_micro_mass = LinearNDInterpolator(list(zip(mic_df['sp_lon'], mic_df['sp_lat'])), mic_df['vansebillemodel_mass_log'])\n",
    "interp_micro_abu = LinearNDInterpolator(list(zip(mic_df['sp_lon'], mic_df['sp_lat'])), mic_df['vansebillemodel_abundance_log'])\n",
    "\n",
    "\n",
    "mass = interp_micro_mass(cygnss_and_interp_df['sp_lon'], cygnss_and_interp_df['sp_lat'])\n",
    "cygnss_and_interp_df['van_sebille_mass'] = mass\n",
    "cygnss_and_interp_df\n",
    "df = cygnss_and_interp_df.iloc[: , 11:]\n",
    "df2 = cygnss_and_interp_df.iloc[: , :2]\n",
    "df['sp_lat'] = df2['sp_lat']\n",
    "df['sp_lon'] = df2['sp_lon']\n",
    "df_mss_ano_micro = df\n",
    "\n",
    "df_mss_ano_micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_var(mic_df, 'vansebillemodel_mass_log')\n",
    "plot_var(mic_df, 'vansebillemodel_abundance_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    plot_var(cygnss_and_interp_df,'diff_nbrcs' )\n",
    "    plot_var(cygnss_and_interp_df,'diff_mss' )\n",
    "    plot_df = pd.DataFrame({'NBRCS_LOG_SCALED' : cygnss_and_interp_df['nbrcs_log'], 'Wind_Speed' : np.sqrt(cygnss_and_interp_df['u10']**2+cygnss_and_interp_df['v10']**2)})\n",
    "    plot_df = plot_df[plot_df.NBRCS_LOG_SCALED > 10]\n",
    "    plot_df = plot_df[plot_df.NBRCS_LOG_SCALED < 23]\n",
    "    plot_df = plot_df[plot_df.Wind_Speed > 0.5]\n",
    "\n",
    "    fig = px.density_heatmap(plot_df, y=\"NBRCS_LOG_SCALED\", x=\"Wind_Speed\", nbinsy=100 , color_continuous_scale=px.colors.sequential.Blackbody  ,title=\"2D Histogram plot of Wind speed and NBRCS in DB scale\")\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    plt.style.use('seaborn-whitegrid')\n",
    "    plt.plot((plot_df['Wind_Speed'].to_numpy()), (plot_df['NBRCS_LOG_SCALED'].to_numpy()), 'o', color='black');\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    era_5_df_one_hour = era_5_df.loc[(era_5_df['hours_since_ref'] == 254122.0)]\n",
    "    cygnss_and_wind_df_one_hour = cygnss_and_interp_df.loc[(cygnss_and_interp_df['hours_since_ref'] == 254122.0)]\n",
    "\n",
    "    plot_wind(era_5_df_one_hour, 'u10', 'v10')\n",
    "    plot_wind(cygnss_and_wind_df_one_hour,  'u10', 'v10',interpolated= True)\n",
    "    \n",
    "    oskar_df_one_hour = oskar_df.loc[(oskar_df['hours_since_ref'] == 254112)]\n",
    "    cygnss_and_wind_df_one_hour = cygnss_and_interp_df.loc[(cygnss_and_interp_df['hours_since_ref'] == 254122.0)]\n",
    "\n",
    "    plot_wind(oskar_df_one_hour, 'u', 'v')\n",
    "    plot_wind(cygnss_and_wind_df_one_hour,  'u', 'v', interpolated= True)\n",
    "\n",
    "    plot_current_df = pd.DataFrame({'NBRCS_LOG_SCALED' : cygnss_and_interp_df['nbrcs_log'], 'Ocean_current_speed_15_meters_depth' : np.sqrt(cygnss_and_interp_df['u']**2+cygnss_and_interp_df['v']**2)})\n",
    "    plot_current_df = plot_current_df[plot_current_df.NBRCS_LOG_SCALED > 10]\n",
    "    plot_current_df = plot_current_df[plot_current_df.NBRCS_LOG_SCALED < 23]\n",
    "    #plot_current_df = plot_current_df[plot_current_df.Ocean_current_speed_15_meters_depth < 1.1]\n",
    "    fig = px.density_heatmap(plot_current_df, y=\"NBRCS_LOG_SCALED\", x=\"Ocean_current_speed_15_meters_depth\", title=\"2D Histogram plot of Ocean current strength at 15 meters depth and NBRCS in DB scale\", color_continuous_scale=px.colors.sequential.Blackbody)\n",
    "    fig.show()\n",
    "\n",
    "    plt.style.use('seaborn-whitegrid')\n",
    "    plt.plot((plot_current_df['Ocean_current_speed_15_meters_depth'].to_numpy()), (plot_current_df['NBRCS_LOG_SCALED'].to_numpy()), 'o', color='black');\n",
    "    \n",
    "    plt.show()\n",
    "    cygnss_and_wind_df_one_hour['total_wind'] = np.sqrt(cygnss_and_wind_df_one_hour['u10']**2 + cygnss_and_wind_df_one_hour['v10']**2)\n",
    "\n",
    "    fig = px.density_mapbox(cygnss_and_wind_df_one_hour, lat='sp_lat', lon='sp_lon', z='total_wind', radius=10,\n",
    "                            center=dict(lat=0, lon=180), zoom=0,\n",
    "                            mapbox_style=\"stamen-terrain\")\n",
    "    fig.show()\n",
    "    plot_var(mic_df, 'vansebillemodel_mass_log')\n",
    "    plot_var(mic_df, 'vansebillemodel_abundance_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}